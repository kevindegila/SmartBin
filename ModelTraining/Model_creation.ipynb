{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/TRAIN/'\n",
    "test_dir = 'data/TEST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12566\n",
      "10000\n",
      "1402\n",
      "1113\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_dir + 'O')))\n",
    "print(len(os.listdir(train_dir + 'R')))\n",
    "print(len(os.listdir(test_dir + 'O')))\n",
    "print(len(os.listdir(test_dir + 'R')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22564 images belonging to 2 classes.\n",
      "Found 2513 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model architecture and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ModelCheckpoint callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./best_model_2.h5', monitor=\"val_acc\", mode=\"max\",\n",
    "                          save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2513/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.7908\n",
      "Epoch 00001: val_acc improved from -inf to 0.88902, saving model to ./best_model_2.h5\n",
      "705/705 [==============================] - 612s 868ms/step - loss: 0.4582 - acc: 0.7910 - val_loss: 0.3135 - val_acc: 0.8890\n",
      "Epoch 2/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8283\n",
      "Epoch 00002: val_acc did not improve from 0.88902\n",
      "705/705 [==============================] - 624s 886ms/step - loss: 0.3920 - acc: 0.8284 - val_loss: 0.3523 - val_acc: 0.8602\n",
      "Epoch 3/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8473\n",
      "Epoch 00003: val_acc did not improve from 0.88902\n",
      "705/705 [==============================] - 621s 881ms/step - loss: 0.3603 - acc: 0.8474 - val_loss: 0.3070 - val_acc: 0.8714\n",
      "Epoch 4/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8554\n",
      "Epoch 00004: val_acc did not improve from 0.88902\n",
      "705/705 [==============================] - 613s 870ms/step - loss: 0.3412 - acc: 0.8555 - val_loss: 0.3399 - val_acc: 0.8690\n",
      "Epoch 5/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8645\n",
      "Epoch 00005: val_acc improved from 0.88902 to 0.89303, saving model to ./best_model_2.h5\n",
      "705/705 [==============================] - 635s 901ms/step - loss: 0.3219 - acc: 0.8645 - val_loss: 0.2774 - val_acc: 0.8930\n",
      "Epoch 6/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.8756\n",
      "Epoch 00006: val_acc did not improve from 0.89303\n",
      "705/705 [==============================] - 625s 887ms/step - loss: 0.3052 - acc: 0.8754 - val_loss: 0.3165 - val_acc: 0.8654\n",
      "Epoch 7/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.8804\n",
      "Epoch 00007: val_acc did not improve from 0.89303\n",
      "705/705 [==============================] - 645s 915ms/step - loss: 0.2926 - acc: 0.8805 - val_loss: 0.3152 - val_acc: 0.8718\n",
      "Epoch 8/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.8883\n",
      "Epoch 00008: val_acc improved from 0.89303 to 0.89784, saving model to ./best_model_2.h5\n",
      "705/705 [==============================] - 619s 878ms/step - loss: 0.2763 - acc: 0.8882 - val_loss: 0.2701 - val_acc: 0.8978\n",
      "Epoch 9/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8955\n",
      "Epoch 00009: val_acc improved from 0.89784 to 0.90465, saving model to ./best_model_2.h5\n",
      "705/705 [==============================] - 628s 890ms/step - loss: 0.2605 - acc: 0.8955 - val_loss: 0.2784 - val_acc: 0.9046\n",
      "Epoch 10/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.8996\n",
      "Epoch 00010: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 611s 867ms/step - loss: 0.2475 - acc: 0.8995 - val_loss: 0.3293 - val_acc: 0.8746\n",
      "Epoch 11/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9068\n",
      "Epoch 00011: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 611s 866ms/step - loss: 0.2344 - acc: 0.9066 - val_loss: 0.2541 - val_acc: 0.9022\n",
      "Epoch 12/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9154\n",
      "Epoch 00012: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 611s 867ms/step - loss: 0.2174 - acc: 0.9154 - val_loss: 0.2848 - val_acc: 0.8890\n",
      "Epoch 13/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9218\n",
      "Epoch 00013: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 612s 868ms/step - loss: 0.2008 - acc: 0.9218 - val_loss: 0.2738 - val_acc: 0.8962\n",
      "Epoch 14/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9283\n",
      "Epoch 00014: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 617s 876ms/step - loss: 0.1846 - acc: 0.9284 - val_loss: 0.3891 - val_acc: 0.8594\n",
      "Epoch 15/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9343\n",
      "Epoch 00015: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 627s 889ms/step - loss: 0.1703 - acc: 0.9342 - val_loss: 0.3527 - val_acc: 0.8842\n",
      "Epoch 16/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9414\n",
      "Epoch 00016: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 626s 888ms/step - loss: 0.1547 - acc: 0.9414 - val_loss: 0.4450 - val_acc: 0.8542\n",
      "Epoch 17/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9480\n",
      "Epoch 00017: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 630s 893ms/step - loss: 0.1403 - acc: 0.9480 - val_loss: 0.2902 - val_acc: 0.8974\n",
      "Epoch 18/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9538\n",
      "Epoch 00018: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 623s 884ms/step - loss: 0.1262 - acc: 0.9539 - val_loss: 0.3541 - val_acc: 0.9046\n",
      "Epoch 19/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9578\n",
      "Epoch 00019: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 626s 888ms/step - loss: 0.1147 - acc: 0.9578 - val_loss: 0.4149 - val_acc: 0.8846\n",
      "Epoch 20/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9626\n",
      "Epoch 00020: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 627s 890ms/step - loss: 0.1032 - acc: 0.9626 - val_loss: 0.3961 - val_acc: 0.8978\n",
      "Epoch 21/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9673\n",
      "Epoch 00021: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 625s 887ms/step - loss: 0.0906 - acc: 0.9673 - val_loss: 0.4372 - val_acc: 0.8934\n",
      "Epoch 22/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9730\n",
      "Epoch 00022: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 630s 894ms/step - loss: 0.0786 - acc: 0.9730 - val_loss: 0.3846 - val_acc: 0.9018\n",
      "Epoch 23/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9747\n",
      "Epoch 00023: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 640s 907ms/step - loss: 0.0713 - acc: 0.9747 - val_loss: 0.4007 - val_acc: 0.9038\n",
      "Epoch 24/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9779\n",
      "Epoch 00024: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 636s 902ms/step - loss: 0.0643 - acc: 0.9778 - val_loss: 0.3971 - val_acc: 0.8974\n",
      "Epoch 25/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9800\n",
      "Epoch 00025: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 633s 898ms/step - loss: 0.0578 - acc: 0.9801 - val_loss: 0.4805 - val_acc: 0.8982\n",
      "Epoch 26/50\n",
      "704/705 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9810\n",
      "Epoch 00026: val_acc did not improve from 0.90465\n",
      "705/705 [==============================] - 627s 890ms/step - loss: 0.0544 - acc: 0.9810 - val_loss: 0.5589 - val_acc: 0.8542\n",
      "Epoch 27/50\n",
      "608/705 [========================>.....] - ETA: 1:25 - loss: 0.0463 - acc: 0.9852"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1fc0ef2c7ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2513\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/Deep-Learning/SmartBin/env/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=int(22564/32),\n",
    "      epochs=50,\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=int(2513/32),\n",
    "      callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
